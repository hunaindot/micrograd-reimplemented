{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from micrograd.engine import Value\n",
    "from graphviz import Digraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for drawing nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trace(root):\n",
    "    nodes, edges = set(), set()\n",
    "    def build(v):\n",
    "        if v not in nodes:\n",
    "            nodes.add(v)\n",
    "            for child in v._prev:\n",
    "                edges.add((child, v))\n",
    "                build(child)\n",
    "    build(root)\n",
    "    return nodes, edges\n",
    "\n",
    "def draw_dot(root, format='svg', rankdir='LR'):\n",
    "    \"\"\"\n",
    "    format: png | svg | ...\n",
    "    rankdir: TB (top to bottom graph) | LR (left to right)\n",
    "    \"\"\"\n",
    "    assert rankdir in ['LR', 'TB']\n",
    "    nodes, edges = trace(root)\n",
    "    dot = Digraph(format=format, graph_attr={'rankdir': rankdir}) #, node_attr={'rankdir': 'TB'})\n",
    "    \n",
    "    for n in nodes:\n",
    "        dot.node(name=str(id(n)), label = \"{ data %.4f | grad %.4f }\" % (n.data, n.grad), shape='record')\n",
    "        if n._op:\n",
    "            dot.node(name=str(id(n)) + n._op, label=n._op)\n",
    "            dot.edge(str(id(n)) + n._op, str(id(n)))\n",
    "    \n",
    "    for n1, n2 in edges:\n",
    "        dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n",
    "    \n",
    "    return dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing of Engine for Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" Forward Pass\n",
      "      x = Value(data=-4.0, grad=0),\n",
      "      z = Value(data=-10.0, grad=0),\n",
      "      q = Value(data=40.0, grad=0),\n",
      "      h = Value(data=100.0, grad=0),\n",
      "      y = Value(data=-20.0, grad=0)\n",
      "      \n",
      "\" Backward Pass\n",
      "      xmg = Value(data=-4.0, grad=46.0),\n",
      "      ymg = Value(data=-20.0, grad=1)\n",
      "      \n"
     ]
    }
   ],
   "source": [
    "# Using Value \n",
    "\n",
    "# Forward Pass\n",
    "x = Value(-4.0)\n",
    "z = 2 * x + 2 + x\n",
    "q = z.relu() + z * x\n",
    "h = (z * z).relu()\n",
    "y = h + q + q * x\n",
    "\n",
    "print(f\"\"\"\" Forward Pass\n",
    "      x = {x},\n",
    "      z = {z},\n",
    "      q = {q},\n",
    "      h = {h},\n",
    "      y = {y}\n",
    "      \"\"\")\n",
    "\n",
    "#Backward pass\n",
    "y.backward()\n",
    "xmg, ymg = x, y\n",
    "\n",
    "print(f\"\"\"\" Backward Pass\n",
    "      xmg = {x},\n",
    "      ymg = {y}\n",
    "      \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" Forward Pass\n",
      "      x = tensor([-4.], dtype=torch.float64, requires_grad=True),\n",
      "      z = tensor([-10.], dtype=torch.float64, grad_fn=<AddBackward0>),\n",
      "      q = tensor([40.], dtype=torch.float64, grad_fn=<AddBackward0>),\n",
      "      h = tensor([100.], dtype=torch.float64, grad_fn=<ReluBackward0>),\n",
      "      y = tensor([-20.], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "      \n",
      "\" Backward Pass\n",
      "      xpt = tensor([-4.], dtype=torch.float64, requires_grad=True), tensor([46.], dtype=torch.float64)\n",
      "      ypt = tensor([-20.], dtype=torch.float64, grad_fn=<AddBackward0>), None\n",
      "      \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hunai\\AppData\\Local\\Temp\\ipykernel_21128\\909692138.py:23: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:494.)\n",
      "  ypt = {y}, {y.grad}\n"
     ]
    }
   ],
   "source": [
    "# Using PyTorch\n",
    "# Forward Pass\n",
    "x = torch.Tensor([-4.0]).double()\n",
    "x.requires_grad = True\n",
    "z = 2 * x + 2 + x\n",
    "q = z.relu() + z * x\n",
    "h = (z * z).relu()\n",
    "y = h + q + q * x\n",
    "\n",
    "print(f\"\"\"\" Forward Pass\n",
    "      x = {x},\n",
    "      z = {z},\n",
    "      q = {q},\n",
    "      h = {h},\n",
    "      y = {y}\n",
    "      \"\"\")\n",
    "\n",
    "y.backward()\n",
    "xpt, ypt = x, y\n",
    "\n",
    "print(f\"\"\"\" Backward Pass\n",
    "      xpt =  {x.grad}\n",
    "      ypt =  {y.grad}\n",
    "      \"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
